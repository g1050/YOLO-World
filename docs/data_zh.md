## å‡†å¤‡ YOLO-World çš„æ•°æ®

### æ¦‚è§ˆ

ä¸ºäº†é¢„è®­ç»ƒ YOLO-Worldï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä»¥ä¸‹è¡¨æ ¼ä¸­åˆ—å‡ºçš„å‡ ä¸ªæ•°æ®é›†ï¼š

| æ•°æ® | æ ·æœ¬æ•° | ç±»å‹ | ç›’æ•° |
| :-- | :-----: | :---:| :---: | 
| Objects365v1 | 609k | detection | 9,621k |
| GQA | 621k | grounding | 3,681k |
| Flickr | 149k | grounding | 641k |
| CC3M-Lite | 245k | image-text | 821k |
 
### æ•°æ®é›†ç›®å½•

We put all data into the `data` directory, such as:  
æˆ‘ä»¬å°†æ‰€æœ‰æ•°æ®æ”¾åœ¨ `data` ç›®å½•ä¸­ï¼Œä¾‹å¦‚ï¼š

```bash
â”œâ”€â”€ coco
â”‚   â”œâ”€â”€ annotations
â”‚   â”œâ”€â”€ lvis
â”‚   â”œâ”€â”€ train2017
â”‚   â”œâ”€â”€ val2017
â”œâ”€â”€ flickr
â”‚   â”œâ”€â”€ annotations
â”‚   â””â”€â”€ images
â”œâ”€â”€ mixed_grounding
â”‚   â”œâ”€â”€ annotations
â”‚   â”œâ”€â”€ images
â”œâ”€â”€ mixed_grounding
â”‚   â”œâ”€â”€ annotations
â”‚   â”œâ”€â”€ images
â”œâ”€â”€ objects365v1
â”‚   â”œâ”€â”€ annotations
â”‚   â”œâ”€â”€ train
â”‚   â”œâ”€â”€ val
```
**NOTE**: We strongly suggest that you check the directories or paths in the dataset part of the config file, especially for the values `ann_file`, `data_root`, and `data_prefix`.  
**æ³¨æ„**: æˆ‘ä»¬å¼ºçƒˆå»ºè®®æ‚¨æ£€æŸ¥é…ç½®æ–‡ä»¶æ•°æ®é›†éƒ¨åˆ†ä¸­çš„ç›®å½•æˆ–è·¯å¾„ï¼Œç‰¹åˆ«æ˜¯ `ann_file`ã€`data_root` å’Œ `data_prefix` è¿™äº›å€¼ã€‚  

We provide the annotations of the pre-training data in the below table:  
æˆ‘ä»¬åœ¨ä¸‹é¢çš„è¡¨æ ¼ä¸­æä¾›äº†é¢„è®­ç»ƒæ•°æ®çš„æ ‡æ³¨ï¼š  

| Data | images | Annotation File |
| :--- | :------| :-------------- |
| Objects365v1 | [`Objects365 train`](https://opendatalab.com/OpenDataLab/Objects365_v1) | [`objects365_train.json`](https://opendatalab.com/OpenDataLab/Objects365_v1) |
| MixedGrounding | [`GQA`](https://nlp.stanford.edu/data/gqa/images.zip) | [`final_mixed_train_no_coco.json`](https://huggingface.co/GLIPModel/GLIP/tree/main/mdetr_annotations/final_mixed_train_no_coco.json) |
| Flickr30k | [`Flickr30k`](https://shannon.cs.illinois.edu/DenotationGraph/) |[`final_flickr_separateGT_train.json`](https://huggingface.co/GLIPModel/GLIP/tree/main/mdetr_annotations/final_flickr_separateGT_train.json) |
| LVIS-minival | [`COCO val2017`](https://cocodataset.org/) | [`lvis_v1_minival_inserted_image_name.json`](https://huggingface.co/GLIPModel/GLIP/blob/main/lvis_v1_minival_inserted_image_name.json) |

**Acknowledgement:** We sincerely thank [GLIP](https://github.com/microsoft/GLIP) and [mdetr](https://github.com/ashkamath/mdetr) for providing the annotation files for pre-training.  
**è‡´è°¢:** æˆ‘ä»¬è¡·å¿ƒæ„Ÿè°¢ [GLIP](https://github.com/microsoft/GLIP) å’Œ [mdetr](https://github.com/ashkamath/mdetr) æä¾›é¢„è®­ç»ƒçš„æ ‡æ³¨æ–‡ä»¶ã€‚  


### æ•°æ®é›†ç±»åˆ«

> å¯¹äºåœ¨å°é—­é›†å¯¹è±¡æ£€æµ‹ä¸Šå¾®è°ƒ YOLO-Worldï¼Œæ¨èä½¿ç”¨ `MultiModalDataset`ã€‚

#### è®¾ç½®ç±»åˆ«/åˆ†ç±»

å¦‚æœæ‚¨ä½¿ç”¨ `COCOæ ¼å¼` çš„è‡ªå®šä¹‰æ•°æ®é›†ï¼Œæ‚¨â€œä¸éœ€è¦â€å®šä¹‰ä¸€ä¸ªç”¨äºè‡ªå®šä¹‰è¯æ±‡/åˆ†ç±»çš„æ•°æ®é›†ç±»åˆ«ã€‚
åœ¨é…ç½®æ–‡ä»¶ä¸­é€šè¿‡ `metainfo=dict(classes=your_classes),` æ˜ç¡®è®¾ç½®ç±»åˆ«æ˜¯ç®€å•çš„ï¼š

```python

coco_train_dataset = dict(
    _delete_=True,
    type='MultiModalDataset',
    dataset=dict(
        type='YOLOv5CocoDataset',
        metainfo=dict(classes=your_classes),
        data_root='data/your_data',
        ann_file='annotations/your_annotation.json',
        data_prefix=dict(img='images/'),
        filter_cfg=dict(filter_empty_gt=False, min_size=32)),
    class_text_path='data/texts/your_class_texts.json',
    pipeline=train_pipeline)
```


For training YOLO-World, we mainly adopt two kinds of dataset classs:  
å¯¹äºè®­ç»ƒ YOLO-Worldï¼Œæˆ‘ä»¬ä¸»è¦é‡‡ç”¨ä¸¤ç§æ•°æ®é›†ç±»ï¼š  

#### 1. `MultiModalDataset`

`MultiModalDataset` is a simple wrapper for pre-defined Dataset Class, such as `Objects365` or `COCO`, which add the texts (category texts) into the dataset instance for formatting input texts.  
`MultiModalDataset` æ˜¯ä¸€ä¸ªé¢„å®šä¹‰æ•°æ®é›†ç±»çš„ç®€å•åŒ…è£…å™¨ï¼Œæ¯”å¦‚ `Objects365` æˆ– `COCO`ï¼Œå®ƒå°†æ–‡æœ¬ï¼ˆç±»åˆ«æ–‡æœ¬ï¼‰æ·»åŠ åˆ°æ•°æ®é›†å®ä¾‹ä¸­ä»¥æ ¼å¼åŒ–è¾“å…¥æ–‡æœ¬ã€‚  

**Text JSON**

The json file is formatted as follows:

```json
[
    ['A_1','A_2'],
    ['B'],
    ['C_1', 'C_2', 'C_3'],
    ...
]
```

We have provided the text json for [`LVIS`](./../data/texts/lvis_v1_class_texts.json), [`COCO`](../data/texts/coco_class_texts.json), and [`Objects365`](../data/texts/obj365v1_class_texts.json)

#### 2. `YOLOv5MixedGroundingDataset`

The `YOLOv5MixedGroundingDataset` extends the `COCO` dataset by supporting loading texts/captions from the json file. It's desgined for `MixedGrounding` or `Flickr30K` with text tokens for each object.  
`YOLOv5MixedGroundingDataset` é€šè¿‡æ”¯æŒä» json æ–‡ä»¶åŠ è½½æ–‡æœ¬/æè¿°æ¥æ‰©å±• `COCO` æ•°æ®é›†ã€‚å®ƒä¸“ä¸º `MixedGrounding` æˆ– `Flickr30K` è®¾è®¡ï¼Œæ¯ä¸ªå¯¹è±¡éƒ½å¸¦æœ‰æ–‡æœ¬æ ‡è®°ã€‚  



### ğŸ”¥ è‡ªå®šä¹‰æ•°æ®é›†

å¯¹äºè‡ªå®šä¹‰æ•°æ®é›†ï¼Œæˆ‘ä»¬å»ºè®®ç”¨æˆ·æ ¹æ®ä½¿ç”¨æƒ…å†µè½¬æ¢æ³¨é‡Šæ–‡ä»¶ã€‚è¯·æ³¨æ„ï¼Œå°†æ³¨é‡Šè½¬æ¢ä¸º**æ ‡å‡†çš„COCOæ ¼å¼**åŸºæœ¬ä¸Šæ˜¯å¿…éœ€çš„ã€‚

1. **å¤§è¯æ±‡é‡ã€å®šä½ã€å¼•ç”¨ï¼š** æ‚¨å¯ä»¥éµå¾ª`MixedGrounding`æ•°æ®é›†çš„æ³¨é‡Šæ ¼å¼ï¼Œè¯¥æ ¼å¼ä¸ºæ¯ä¸ªå¯¹è±¡æ·»åŠ äº†`caption`å’Œ`tokens_positive`ä»¥åˆ†é…æ–‡æœ¬ã€‚æ–‡æœ¬å¯ä»¥æ˜¯ä¸€ä¸ªç±»åˆ«æˆ–åè¯çŸ­è¯­ã€‚

2. **è‡ªå®šä¹‰è¯æ±‡ï¼ˆå›ºå®šï¼‰ï¼š** æ‚¨å¯ä»¥é‡‡ç”¨`MultiModalDataset`åŒ…è£…å™¨ï¼Œå¦‚`Objects365`ï¼Œå¹¶ä¸ºæ‚¨çš„è‡ªå®šä¹‰ç±»åˆ«åˆ›å»ºä¸€ä¸ª**æ–‡æœ¬json**ã€‚


### CC3M ä¼ªæ ‡æ³¨

ä»¥ä¸‹æ ‡æ³¨æ˜¯æ ¹æ®æˆ‘ä»¬è®ºæ–‡ä¸­çš„è‡ªåŠ¨æ ‡æ³¨è¿‡ç¨‹ç”Ÿæˆçš„ã€‚å¹¶ä¸”æˆ‘ä»¬æ ¹æ®è¿™äº›æ ‡æ³¨æŠ¥å‘Šäº†ç»“æœã€‚

è¦ä½¿ç”¨ CC3M æ ‡æ³¨ï¼Œæ‚¨é¦–å…ˆéœ€è¦å‡†å¤‡ `CC3M` å›¾åƒã€‚

| Data | Images | Boxes | File |
| :--: | :----: | :---: | :---: |
| CC3M-246K | 246,363 | 820,629 | [Download ğŸ¤—](https://huggingface.co/wondervictor/YOLO-World/blob/main/cc3m_pseudo_annotations.json) |
| CC3M-500K | 536,405 | 1,784,405| [Download ğŸ¤—](https://huggingface.co/wondervictor/YOLO-World/blob/main/cc3m_pseudo_500k_annotations.json) |
| CC3M-750K | 750,000 | 4,504,805 | [Download ğŸ¤—](https://huggingface.co/wondervictor/YOLO-World/blob/main/cc3m_pseudo_750k_annotations.json) |
<div align="center">
<img src="./assets/yolo_logo.png" width=60%>
<br>
<a href="https://scholar.google.com/citations?hl=zh-CN&user=PH8rJHYAAAAJ">Tianheng Cheng</a><sup><span>2,3,*</span></sup>, 
<a href="https://linsong.info/">Lin Song</a><sup><span>1,ğŸ“§,*</span></sup>,
<a href="https://yxgeee.github.io/">Yixiao Ge</a><sup><span>1,ğŸŒŸ,2</span></sup>,
<a href="http://eic.hust.edu.cn/professor/liuwenyu/"> Wenyu Liu</a><sup><span>3</span></sup>,
<a href="https://xwcv.github.io/">Xinggang Wang</a><sup><span>3,ğŸ“§</span></sup>,
<a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en">Ying Shan</a><sup><span>1,2</span></sup>
</br>

\* Equal contribution ğŸŒŸ Project lead ğŸ“§ Corresponding author

<sup>1</sup> Tencent AI Lab,  <sup>2</sup> ARC Lab, Tencent PCG
<sup>3</sup> Huazhong University of Science and Technology
<br>
<div>

[![arxiv paper](https://img.shields.io/badge/Project-Page-green)](https://wondervictor.github.io/)
[![arxiv paper](https://img.shields.io/badge/arXiv-Paper-red)](https://arxiv.org/abs/2401.17270)
<a href="https://colab.research.google.com/github/AILab-CVC/YOLO-World/blob/master/inference.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>
[![demo](https://img.shields.io/badge/ğŸ¤—HugginngFace-Spaces-orange)](https://huggingface.co/spaces/stevengrove/YOLO-World)
[![Replicate](https://replicate.com/zsxkib/yolo-world/badge)](https://replicate.com/zsxkib/yolo-world)
[![hfpaper](https://img.shields.io/badge/ğŸ¤—HugginngFace-Paper-yellow)](https://huggingface.co/papers/2401.17270)
[![license](https://img.shields.io/badge/License-GPLv3.0-blue)](LICENSE)
[![yoloworldseg](https://img.shields.io/badge/YOLOWorldxEfficientSAM-ğŸ¤—Spaces-orange)](https://huggingface.co/spaces/SkalskiP/YOLO-World)
[![yologuide](https://img.shields.io/badge/ğŸ“–Notebook-roboflow-purple)](https://supervision.roboflow.com/develop/notebooks/zero-shot-object-detection-with-yolo-world)
[![deploy](https://media.roboflow.com/deploy.svg)](https://inference.roboflow.com/foundation/yolo_world/)

</div>
</div>

## Notice

**YOLO-World is still under active development!**

We recommend that everyone **use English to communicate on issues**, as this helps developers from around the world discuss, share experiences, and answer questions together.

For business licensing and other related inquiries, don't hesitate to contact `yixiaoge@tencent.com`.

## ğŸ”¥ æ›´æ–° 
`[2025-2-8]:` æˆ‘ä»¬å‘å¸ƒäº†æ–°çš„ YOLO-World-V2.1ï¼ŒåŒ…æ‹¬æ–°çš„é¢„è®­ç»ƒæƒé‡å’Œå›¾åƒæç¤ºçš„è®­ç»ƒä»£ç ã€‚è¯·æŸ¥çœ‹ [YOLO-World-V2.1åšå®¢](./docs/update_20250123.md) è·å–è¯¦ç»†ä¿¡æ¯ã€‚\
`[2024-11-5]`: æˆ‘ä»¬æ›´æ–°äº† `YOLO-World-Image`ï¼Œæ‚¨å¯ä»¥åœ¨ HuggingFace [YOLO-World-Imageï¼ˆé¢„è§ˆç‰ˆï¼‰](https://huggingface.co/spaces/wondervictor/YOLO-World-Image) å°è¯•å®ƒã€‚è¿™æ˜¯ä¸€ä¸ª*é¢„è§ˆ*ç‰ˆæœ¬ï¼Œæˆ‘ä»¬ä»åœ¨æ”¹è¿›ä¸­ï¼å…³äºè®­ç»ƒå’Œå°‘é‡æ ·æœ¬æ¨ç†çš„è¯¦ç»†æ–‡æ¡£å³å°†å‘å¸ƒã€‚\
`[2024-7-8]`: YOLO-World ç°å·²é›†æˆåˆ° [ComfyUI](https://github.com/StevenGrove/ComfyUI-YOLOWorld) ä¸­ï¼å¿«æ¥å°è¯•å°† YOLO-World æ·»åŠ åˆ°æ‚¨çš„å·¥ä½œæµç¨‹ä¸­ï¼æ‚¨å¯ä»¥åœ¨ [StevenGrove/ComfyUI-YOLOWorld](https://github.com/StevenGrove/ComfyUI-YOLOWorld) è®¿é—®å®ƒï¼\
`[2024-5-18]:` YOLO-World æ¨¡å‹å·²ä¸ FiftyOne è®¡ç®—æœºè§†è§‰å·¥å…·åŒ…é›†æˆï¼Œç”¨äºè·¨å›¾åƒå’Œè§†é¢‘æ•°æ®é›†çš„æµç•…å¼€æ”¾è¯æ±‡æ¨ç†ã€‚\
`[2024-5-16]:` å˜¿ï¼Œå¤§å®¶å¥½ï¼å¥½ä¹…ä¸è§ï¼æ­¤æ¬¡æ›´æ–°åŒ…å« (1) [å¾®è°ƒæŒ‡å—](https://github.com/AILab-CVC/YOLO-World?#highlights--introduction) å’Œ (2) [TFLite å¯¼å‡º](./docs/tflite_deploy.md) ä¸ INT8 é‡åŒ–ã€‚\
`[2024-5-9]:` æ­¤æ¬¡æ›´æ–°åŒ…å«çœŸæ­£çš„ [`é‡æ–°å‚æ•°åŒ–`](./docs/reparameterize.md) ğŸª„ï¼Œå®ƒæ›´é€‚åˆåœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¹¶æé«˜äº†è®­ç»ƒ/æ¨ç†æ•ˆç‡ ğŸš€ï¼\
`[2024-4-28]:` å¥½ä¹…ä¸è§ï¼æ­¤æ¬¡æ›´æ–°åŒ…å«é”™è¯¯ä¿®å¤å’Œæ”¹è¿›ï¼š(1) ONNX æ¼”ç¤ºï¼›(2) å›¾åƒæ¼”ç¤ºï¼ˆæ”¯æŒå¼ é‡è¾“å…¥ï¼‰ï¼›(3) æ–°çš„é¢„è®­ç»ƒæ¨¡å‹ï¼›(4) å›¾åƒæç¤ºï¼›(5) å¾®è°ƒ/éƒ¨ç½²çš„ç®€åŒ–ç‰ˆæœ¬ï¼›(6) å®‰è£…æŒ‡å—ï¼ˆåŒ…æ‹¬ `requirements.txt`ï¼‰ã€‚\
`[2024-3-28]:` æˆ‘ä»¬æä¾›ï¼š(1) æ›´å¤šé«˜åˆ†è¾¨ç‡çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä¾‹å¦‚ Sã€Mã€Xï¼‰([#142](https://github.com/AILab-CVC/YOLO-World/issues/142))ï¼›(2) å¸¦æœ‰ CLIP-Large æ–‡æœ¬ç¼–ç å™¨çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬åˆæ­¥ä¿®å¤äº†**æ²¡æœ‰ `mask-refine` çš„å¾®è°ƒ**å¹¶æ¢ç´¢äº†æ–°çš„å¾®è°ƒè®¾ç½® ([#160](https://github.com/AILab-CVC/YOLO-World/issues/160),[#76](https://github.com/AILab-CVC/YOLO-World/issues/76))ã€‚æ­¤å¤–ï¼Œä½¿ç”¨ `mask-refine` å¾®è°ƒ YOLO-World ä¹Ÿå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œæ›´å¤šè¯¦æƒ…è¯·æŸ¥çœ‹ [configs/finetune_coco](./configs/finetune_coco/)ã€‚\
`[2024-3-16]:` æˆ‘ä»¬ä¿®å¤äº†å…³äºæ¼”ç¤ºçš„é”™è¯¯ ([#110](https://github.com/AILab-CVC/YOLO-World/issues/110),[#94](https://github.com/AILab-CVC/YOLO-World/issues/94),[#129](https://github.com/AILab-CVC/YOLO-World/issues/129), [#125](https://github.com/AILab-CVC/YOLO-World/issues/125))ï¼ŒåŒ…æ‹¬åˆ†å‰²æ©ç çš„å¯è§†åŒ–ï¼Œå¹¶å‘å¸ƒäº†æ”¯æŒæç¤ºè°ƒæ•´ã€æ–‡æœ¬æç¤ºå’Œå›¾åƒæç¤ºçš„ [**YOLO-World with Embeddings**](./docs/prompt_yolo_world.md)ã€‚\
`[2024-3-3]:` æˆ‘ä»¬æ·»åŠ äº†**é«˜åˆ†è¾¨ç‡ YOLO-World**ï¼Œæ”¯æŒ `1280x1280` åˆ†è¾¨ç‡ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œå¯¹å°ç‰©ä½“çš„æ›´å¥½æ€§èƒ½ï¼\
`[2024-2-29]:` æˆ‘ä»¬å‘å¸ƒäº†æœ€æ–°ç‰ˆæœ¬çš„ [**YOLO-World-v2**](./docs/updates.md)ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ›´å¿«çš„é€Ÿåº¦ï¼æˆ‘ä»¬å¸Œæœ›ç¤¾åŒºèƒ½åŠ å…¥æˆ‘ä»¬ï¼Œå…±åŒæ”¹è¿› YOLO-Worldï¼\
`[2024-2-28]:` æ¿€åŠ¨åœ°å®£å¸ƒ YOLO-World å·²è¢« **CVPR 2024** æ¥å—ï¼æˆ‘ä»¬å°†ç»§ç»­ä½¿ YOLO-World æ›´å¿«æ›´å¼ºï¼ŒåŒæ—¶ä½¿å…¶æ›´æ˜“äºä½¿ç”¨ã€‚\
`[2024-2-22]:` æˆ‘ä»¬è¡·å¿ƒæ„Ÿè°¢ [RoboFlow](https://roboflow.com/) å’Œ [@Skalskip92](https://twitter.com/skalskip92) åˆ¶ä½œçš„å…³äº YOLO-World çš„ [**è§†é¢‘æŒ‡å—**](https://www.youtube.com/watch?v=X7gKBGVz4vs)ï¼Œå¹²å¾—å¥½ï¼\
`[2024-2-18]:` æˆ‘ä»¬æ„Ÿè°¢ [@Skalskip92](https://twitter.com/skalskip92) é€šè¿‡è¿æ¥ YOLO-World å’Œ EfficientSAM å¼€å‘äº†ç²¾å½©çš„åˆ†å‰²æ¼”ç¤ºã€‚æ‚¨ç°åœ¨å¯ä»¥åœ¨ [ğŸ¤— HuggingFace Spaces](https://huggingface.co/spaces/SkalskiP/YOLO-World) å°è¯•å®ƒã€‚\
`[2024-2-17]:` YOLO-World æœ€å¤§çš„æ¨¡å‹ **X** å·²å‘å¸ƒï¼Œå®ç°äº†æ›´å¥½çš„é›¶æ ·æœ¬æ€§èƒ½ï¼\
`[2024-2-17]:` æˆ‘ä»¬ç°åœ¨å‘å¸ƒäº† **YOLO-World-Seg** çš„ä»£ç å’Œæ¨¡å‹ï¼YOLO-World ç°åœ¨æ”¯æŒå¼€æ”¾è¯æ±‡/é›¶æ ·æœ¬å¯¹è±¡åˆ†å‰²ï¼\
`[2024-2-15]:` å‘å¸ƒäº†å¸¦æœ‰ CC3M-Lite çš„é¢„è®­ç»ƒ YOLO-World-Lï¼\
`[2024-2-14]:` æˆ‘ä»¬æä¾›äº†ç”¨äºå›¾åƒæˆ–ç›®å½•æ¨ç†çš„ [`image_demo`](demo.py)ã€‚\
`[2024-2-10]:` æˆ‘ä»¬æä¾›äº†ç”¨äºåœ¨ COCO æ•°æ®é›†æˆ–è‡ªå®šä¹‰æ•°æ®é›†ä¸Šå¾®è°ƒ YOLO-World çš„ [å¾®è°ƒ](./docs/finetuning.md) å’Œ [æ•°æ®](./docs/data.md) è¯¦æƒ…ï¼\
`[2024-2-3]:` æˆ‘ä»¬ç°åœ¨æ”¯æŒåœ¨ä»“åº“ä¸­çš„ `Gradio` æ¼”ç¤ºï¼Œæ‚¨å¯ä»¥åœ¨è‡ªå·±çš„è®¾å¤‡ä¸Šæ„å»º YOLO-World æ¼”ç¤ºï¼\
`[2024-2-1]:` æˆ‘ä»¬ç°åœ¨å·²å‘å¸ƒ YOLO-World çš„ä»£ç å’Œæƒé‡ï¼\
`[2024-2-1]:` æˆ‘ä»¬åœ¨ [HuggingFace ğŸ¤—](https://huggingface.co/spaces/stevengrove/YOLO-World) ä¸Šéƒ¨ç½²äº† YOLO-World æ¼”ç¤ºï¼Œæ‚¨ç°åœ¨å¯ä»¥å°è¯•å®ƒï¼\
`[2024-1-31]:` æˆ‘ä»¬å¾ˆå…´å¥‹åœ°æ¨å‡º **YOLO-World**ï¼Œè¿™æ˜¯ä¸€æ¬¾å°–ç«¯çš„å®æ—¶å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹å™¨ã€‚


## TODO

YOLO-World is under active development and please stay tuned â˜•ï¸! 
If you have suggestionsğŸ“ƒ or ideasğŸ’¡,**we would love for you to bring them up in the [Roadmap](https://github.com/AILab-CVC/YOLO-World/issues/109)** â¤ï¸!
> YOLO-World ç›®å‰æ­£åœ¨ç§¯æå¼€å‘ä¸­ğŸ“ƒï¼Œå¦‚æœä½ æœ‰å»ºè®®æˆ–è€…æƒ³æ³•ğŸ’¡ï¼Œ**æˆ‘ä»¬éå¸¸å¸Œæœ›æ‚¨åœ¨ [Roadmap](https://github.com/AILab-CVC/YOLO-World/issues/109) ä¸­æå‡ºæ¥** â¤ï¸ï¼

## [FAQ (Frequently Asked Questions)](https://github.com/AILab-CVC/YOLO-World/discussions/149)

We have set up an FAQ about YOLO-World in the discussion on GitHub. We hope everyone can raise issues or solutions during use here, and we also hope that everyone can quickly find solutions from it.

> æˆ‘ä»¬åœ¨GitHubçš„discussionä¸­å»ºç«‹äº†å…³äºYOLO-Worldçš„å¸¸è§é—®ç­”ï¼Œè¿™é‡Œå°†æ”¶é›†ä¸€äº›å¸¸è§é—®é¢˜ï¼ŒåŒæ—¶å¤§å®¶å¯ä»¥åœ¨æ­¤æå‡ºä½¿ç”¨ä¸­çš„é—®é¢˜æˆ–è€…è§£å†³æ–¹æ¡ˆï¼Œä¹Ÿå¸Œæœ›å¤§å®¶èƒ½å¤Ÿä»ä¸­å¿«é€Ÿå¯»æ‰¾åˆ°è§£å†³æ–¹æ¡ˆ


## Highlights & Introduction

æœ¬ä»“åº“åŒ…å«äº†YOLO-Worldçš„PyTorchå®ç°ã€é¢„è®­ç»ƒæƒé‡ä»¥åŠé¢„è®­ç»ƒ/å¾®è°ƒä»£ç ã€‚  
This repo contains the PyTorch implementation, pre-trained weights, and pre-training/fine-tuning code for YOLO-World.

* YOLO-Worldåœ¨åŒ…æ‹¬æ£€æµ‹ã€å®šä½å’Œå›¾åƒ-æ–‡æœ¬æ•°æ®é›†çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒã€‚  
* YOLO-World is pre-trained on large-scale datasets, including detection, grounding, and image-text datasets.

* YOLO-Worldæ˜¯ä¸‹ä¸€ä»£YOLOæ£€æµ‹å™¨ï¼Œå…·æœ‰å¼ºå¤§çš„å¼€æ”¾è¯æ±‡æ£€æµ‹èƒ½åŠ›å’Œå®šä½èƒ½åŠ›ã€‚  
* YOLO-World is the next-generation YOLO detector, with a strong open-vocabulary detection capability and grounding ability.

* YOLO-Worldæå‡ºäº†ä¸€ä¸ªé«˜æ•ˆçš„ç”¨æˆ·è¯æ±‡æ¨ç†èŒƒå¼â€”â€”*å…ˆæç¤ºåæ£€æµ‹*ï¼Œå®ƒå°†è¯æ±‡åµŒå…¥ä½œä¸ºå‚æ•°é‡æ–°å‚æ•°åŒ–åˆ°æ¨¡å‹ä¸­ï¼Œå®ç°äº†å“è¶Šçš„æ¨ç†é€Ÿåº¦ã€‚æ‚¨å¯ä»¥åœ¨æˆ‘ä»¬çš„[åœ¨çº¿æ¼”ç¤º](https://huggingface.co/spaces/stevengrove/YOLO-World)ä¸­å°è¯•å¯¼å‡ºæ‚¨è‡ªå·±çš„æ£€æµ‹æ¨¡å‹ï¼Œæ— éœ€é¢å¤–è®­ç»ƒæˆ–å¾®è°ƒï¼  
* YOLO-World presents a *prompt-then-detect* paradigm for efficient user-vocabulary inference, which re-parameterizes vocabulary embeddings as parameters into the model and achieve superior inference speed. You can try to export your own detection model without extra training or fine-tuning in our [online demo](https://huggingface.co/spaces/stevengrove/YOLO-World)!


<div align="center">
<img width=800px src="./assets/yolo_arch.png">
</div>

### Zero-shot Evaluation Results for Pre-trained Models
æˆ‘ä»¬ä»¥é›¶æ ·æœ¬æ–¹å¼åœ¨LVISã€LVIS-miniå’ŒCOCOä¸Šè¯„ä¼°æ‰€æœ‰YOLO-World-V2.1æ¨¡å‹ï¼Œå¹¶ä¸å‰ä¸€ç‰ˆæœ¬è¿›è¡Œæ¯”è¾ƒï¼ˆæ”¹è¿›ç‚¹ä»¥ä¸Šæ ‡å½¢å¼æ ‡æ³¨ï¼‰ã€‚  
We evaluate all YOLO-World-V2.1 models on LVIS, LVIS-mini, and COCO in the zero-shot manner, and compare with the previous version (the improvements are annotated in the superscripts).


<table>
    <tr>
        <th rowspan="2">Model</th><th rowspan="2">Resolution</th><th colspan="4" style="border-right: 1px solid">LVIS AP</th><th colspan="4">LVIS-mini</th><th colspan="4" style="border-left: 1px solid">COCO</th>
    </tr>
        <td>AP</td><td>AP<sub>r</sub></td><td>AP<sub>c</sub></td><td style="border-right: 1px solid">AP<sub>f</sub></td><td>AP</td><td>AP<sub>r</sub></td><td>AP<sub>c</sub></td><td>AP<sub>f</sub></td><td style="border-left: 1px solid">AP</td><td>AP<sub>50</sub></td><td>AP<sub>75</sub></td>
    <tr style="border-top: 2px solid">
        <td>YOLO-World-S</td><td>640</td><td>18.5<sup>+1.2</sup></td><td>12.6</td><td>15.8</td><td style="border-right: 1px solid">24.1</td><td>23.6<sup>+0.9</sup></td><td>16.4</td><td>21.5</td><td>26.6</td><td style="border-left: 1px solid">36.6</td><td>51.0</td><td>39.7</td>
    </tr>
    <tr>
        <td>YOLO-World-S</td><td>1280</td><td>19.7<sup>+0.9</sup></td><td>13.5</td><td>16.3</td><td style="border-right: 1px solid">26.3</td><td>25.5<sup>+1.4</sup></td><td>19.1</td><td>22.6</td><td>29.3</td><td style="border-left: 1px solid">38.2</td><td>54.2</td><td>41.6</td>
    </tr>
    <tr style="border-top: 2px solid">
        <td>YOLO-World-M</td><td>640</td><td>24.1<sup>+0.6</sup></td><td>16.9</td><td>21.1</td><td style="border-right: 1px solid">30.6</td><td>30.6<sup>+0.6</sup></td><td>19.7</td><td>29.0</td><td>34.1</td><td style="border-left: 1px solid">43.0</td><td>58.6</td><td>46.7</td>
    </tr>
    <tr>
        <td>YOLO-World-M</td><td>1280</td><td>26.0<sup>+0.7</sup></td><td>19.9</td><td>22.5</td><td style="border-right: 1px solid">32.7</td><td>32.7<sup>+1.1</sup></td><td>24.4</td><td>30.2</td><td>36.4</td><td style="border-left: 1px solid">43.8</td><td>60.3</td><td>47.7</td>
    </tr>
    <tr style="border-top: 2px solid">
        <td>YOLO-World-L</td><td>640</td><td>26.8<sup>+0.7</sup></td><td>19.8</td><td>23.6</td><td style="border-right: 1px solid">33.4</td><td>33.8<sup>+0.9</sup></td><td>24.5</td><td>32.3</td><td>36.8</td><td style="border-left: 1px solid">44.9</td><td>60.4</td><td>48.9</td>
    </tr>
    <tr>
        <td>YOLO-World-L</td><td>800</td><td>28.3</td><td>22.5</td><td>24.4</td><td style="border-right: 1px solid">35.1</td><td>35.2</td><td>27.8</td><td>32.6</td><td>38.8</td><td style="border-left: 1px solid">47.4</td><td>63.3</td><td>51.8</td>
    </tr>
    <tr>
        <td>YOLO-World-L</td><td>1280</td><td>28.7<sup>+1.1</sup></td><td>22.9</td><td>24.9</td><td style="border-right: 1px solid">35.4</td><td>35.5<sup>+1.2</sup></td><td>24.4</td><td>34.0</td><td>38.8</td><td style="border-left: 1px solid">46.0</td><td>62.5</td><td>50.0</td>
    </tr>
    <tr style="border-top: 2px solid">
        <td>YOLO-World-X</td><td>640</td><td>28.6<sup>+0.2</sup></td><td>22.0</td><td>25.6</td><td style="border-right: 1px solid">34.9</td><td>35.8<sup>+0.4</sup></td><td>31.0</td><td>33.7</td><td>38.5</td><td style="border-left: 1px solid">46.7</td><td>62.5</td><td>51.0</td>
    </tr>
    <tr>
        <td colspan="13">YOLO-World-X-1280 is coming soon.</td>
    </tr>
</table>

### Model Card

<table>
    <tr>
        <th>Model</th><th>Resolution</th><th>Training</th><th>Data</th><th>Model Weights</th>
    </tr>
    <tr style="border-top: 2px solid">
        <td>YOLO-World-S</td><td>640</td><td>PT (100e)</td><td>O365v1+GoldG+CC-LiteV2</td><td><a href="https://huggingface.co/wondervictor/YOLO-World-V2.1/resolve/main/x_stage1-62b674ad.pth"> ğŸ¤— HuggingFace</a></td>
    </tr>
    <tr>
        <td>YOLO-World-S</td><td>1280</td><td>CPT (40e)</td><td>O365v1+GoldG+CC-LiteV2</td><td><a href="https://huggingface.co/wondervictor/YOLO-World-V2.1/resolve/main/s_stage2-4466ab94.pth"> ğŸ¤— HuggingFace</a></td>
    </tr>
    <tr style="border-top: 2px solid">
        <td>YOLO-World-M</td><td>640</td><td>PT (100e)</td><td>O365v1+GoldG+CC-LiteV2</td><td><a href="https://huggingface.co/wondervictor/YOLO-World-V2.1/resolve/main/m_stage1-7e1e5299.pth"> ğŸ¤— HuggingFace</a></td>
    </tr>
    <tr>
        <td>YOLO-World-M</td><td>1280</td><td>CPT (40e)</td><td>O365v1+GoldG+CC-LiteV2</td><td><a href="https://huggingface.co/wondervictor/YOLO-World-V2.1/resolve/main/m_stage2-9987dcb1.pth"> ğŸ¤— HuggingFace</a></td>
    </tr>
    <tr style="border-top: 2px solid">
        <td>YOLO-World-L</td><td>640</td><td>PT (100e)</td><td>O365v1+GoldG+CC-LiteV2</td><td><a href="https://huggingface.co/wondervictor/YOLO-World-V2.1/resolve/main/l_stage1-7d280586.pth"> ğŸ¤— HuggingFace</a></td>
    </tr>
    <tr>
        <td>YOLO-World-L</td><td>800 / 1280</td><td>CPT (40e)</td><td>O365v1+GoldG+CC-LiteV2</td><td><a href="https://huggingface.co/wondervictor/YOLO-World-V2.1/resolve/main/l_stage2-b3e3dc3f.pth"> ğŸ¤— HuggingFace</a></td>
    </tr>
    <tr style="border-top: 2px solid">
        <td>YOLO-World-X</td><td>640</td><td>PT (100e)</td><td>O365v1+GoldG+CC-LiteV2</td><td><a href="https://huggingface.co/wondervictor/YOLO-World-V2.1/resolve/main/x_stage1-62b674ad.pth"> ğŸ¤— HuggingFace</a></td>
    </tr>
</table>

**Notes:**
* PT: Pre-training, CPT: continuing pre-training
* CC-LiteV2: the newly-annotated CC3M subset, including 250k images.


## Getting started

### 1. Installation

YOLO-World is developed based on `torch==1.11.0` `mmyolo==0.6.0` and `mmdetection==3.0.0`. Check more details about `requirements` and `mmcv` in [docs/installation](./docs/installation.md).

#### Clone Project 

```bash
git clone --recursive https://github.com/AILab-CVC/YOLO-World.git
```
#### Install

```bash
# å®‰è£… torch å’Œ wheel åŒ…ï¼Œ-q å‚æ•°è¡¨ç¤ºå®‰é™æ¨¡å¼ï¼Œå³å‡å°‘ä¸å¿…è¦çš„è¾“å‡º
pip install torch wheel -q
# å®‰è£…å½“å‰ç›®å½•ä¸‹çš„é¡¹ç›®ï¼Œ-e å‚æ•°è¡¨ç¤ºä»¥å¼€å‘æ¨¡å¼å®‰è£…ï¼Œå…è®¸ä½ ä¿®æ”¹ä»£ç å¹¶ç«‹å³çœ‹åˆ°æ›´æ”¹
pip install -e .
```

### 2. Preparing Data

We provide the details about the pre-training data in [docs/data](./docs/data.md).


## Training & Evaluation

æˆ‘ä»¬é‡‡ç”¨ [mmyolo](https://github.com/open-mmlab/mmyolo) çš„é»˜è®¤[è®­ç»ƒ](./tools/train.py)æˆ–[è¯„ä¼°](./tools/test.py)è„šæœ¬ã€‚  
We adopt the default [training](./tools/train.py) or [evaluation](./tools/test.py) scripts of [mmyolo](https://github.com/open-mmlab/mmyolo).  
æˆ‘ä»¬åœ¨ `configs/pretrain` å’Œ `configs/finetune_coco` æä¾›äº†é¢„è®­ç»ƒå’Œå¾®è°ƒçš„é…ç½®ã€‚  
We provide the configs for pre-training and fine-tuning in `configs/pretrain` and `configs/finetune_coco`.  
è®­ç»ƒ YOLO-World å¾ˆç®€å•ï¼š  
Training YOLO-World is easy:  

```bash
chmod +x tools/dist_train.sh
# sample command for pre-training, use AMP for mixed-precision training
./tools/dist_train.sh configs/pretrain/yolo_world_l_t2i_bn_2e-4_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py 8 --amp
```
**NOTE:** YOLO-World is pre-trained on 4 nodes with 8 GPUs per node (32 GPUs in total). For pre-training, the `node_rank` and `nnodes` for multi-node training should be specified. 

Evaluating YOLO-World is also easy:

```bash
chmod +x tools/dist_test.sh
./tools/dist_test.sh path/to/config path/to/weights 8
```

**NOTE:** We mainly evaluate the performance on LVIS-minival for pre-training.

## Fine-tuning YOLO-World

<div align="center">
<img src="./assets/finetune_yoloworld.png" width=800px>
</div>


<div align="center">
<b><p>Chose your pre-trained YOLO-World and Fine-tune it!</p></b> 
</div>


YOLO-World æ”¯æŒ**é›¶æ ·æœ¬æ¨ç†**ï¼Œä»¥åŠä¸‰ç§ç±»å‹çš„**å¾®è°ƒé…æ–¹**ï¼š**(1) å¸¸è§„å¾®è°ƒ**ï¼Œ**(2) æç¤ºå¾®è°ƒ**ï¼Œå’Œ**(3) é‡å‚æ•°åŒ–å¾®è°ƒ**ã€‚
YOLO-World supports **zero-shot inference**, and three types of **fine-tuning recipes**: **(1) normal fine-tuning**, **(2) prompt tuning**, and **(3) reparameterized fine-tuning**.  

* å¸¸è§„å¾®è°ƒï¼šæˆ‘ä»¬åœ¨ [docs/fine-tuning](./docs/finetuning.md) æä¾›äº†æœ‰å…³ YOLO-World å¾®è°ƒçš„è¯¦ç»†ä¿¡æ¯ã€‚
* Normal Fine-tuning: we provide the details about fine-tuning YOLO-World in [docs/fine-tuning](./docs/finetuning.md).

* æç¤ºå¾®è°ƒï¼šæˆ‘ä»¬åœ¨ [docs/prompt_yolo_world](./docs/prompt_yolo_world.md) æä¾›äº†æ›´å¤šå…³äºæç¤ºå¾®è°ƒçš„è¯¦ç»†ä¿¡æ¯ã€‚
* Prompt Tuning: we provide more details about prompt tuning in [docs/prompt_yolo_world](./docs/prompt_yolo_world.md).

* é‡å‚æ•°åŒ–å¾®è°ƒï¼šé‡å‚æ•°åŒ–çš„ YOLO-World æ›´é€‚åˆäºè¿œç¦»é€šç”¨åœºæ™¯çš„ç‰¹å®šé¢†åŸŸã€‚æ‚¨å¯ä»¥åœ¨ [docs/reparameterize](./docs/reparameterize.md) ä¸­æ‰¾åˆ°æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚
* Reparameterized Fine-tuning: the reparameterized YOLO-World is more suitable for specific domains far from generic scenes. You can find more details in [docs/reparameterize](./docs/reparameterize.md).

## Deployment

We provide the details about deployment for downstream applications in [docs/deployment](./docs/deploy.md).
You can directly download the ONNX model through the online [demo](https://huggingface.co/spaces/stevengrove/YOLO-World) in Huggingface Spaces ğŸ¤—.

- [x] ONNX export and demo: [docs/deploy](https://github.com/AILab-CVC/YOLO-World/blob/master/docs/deploy.md)
- [x] TFLite and INT8 Quantization: [docs/tflite_deploy](https://github.com/AILab-CVC/YOLO-World/blob/master/docs/tflite_deploy.md)
- [ ] TensorRT: coming soon.
- [ ] C++: coming soon.

## Demo

See [`demo`](./demo) for more details

- [x] `gradio_demo.py`: Gradio demo, ONNX export
- [x] `image_demo.py`: inference with images or a directory of images
- [x] `simple_demo.py`: a simple demo of YOLO-World, using `array` (instead of path as input).
- [x] `video_demo.py`: inference YOLO-World on videos.
- [x] `inference.ipynb`: jupyter notebook for YOLO-World.
- [x] [Google Colab Notebook](https://colab.research.google.com/drive/1F_7S5lSaFM06irBCZqjhbN7MpUXo6WwO?usp=sharing): We sincerely thank [Onuralp](https://github.com/onuralpszr) for sharing the [Colab Demo](https://colab.research.google.com/drive/1F_7S5lSaFM06irBCZqjhbN7MpUXo6WwO?usp=sharing), you can have a try ğŸ˜Šï¼

## Acknowledgement

We sincerely thank [mmyolo](https://github.com/open-mmlab/mmyolo), [mmdetection](https://github.com/open-mmlab/mmdetection), [GLIP](https://github.com/microsoft/GLIP), and [transformers](https://github.com/huggingface/transformers) for providing their wonderful code to the community!

## Citations
If you find YOLO-World is useful in your research or applications, please consider giving us a star ğŸŒŸ and citing it.

```bibtex
@inproceedings{Cheng2024YOLOWorld,
  title={YOLO-World: Real-Time Open-Vocabulary Object Detection},
  author={Cheng, Tianheng and Song, Lin and Ge, Yixiao and Liu, Wenyu and Wang, Xinggang and Shan, Ying},
  booktitle={Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)},
  year={2024}
}
```

## Licence
YOLO-World is under the GPL-v3 Licence and is supported for commercial usage. If you need a commercial license for YOLO-World, please feel free to contact us.
